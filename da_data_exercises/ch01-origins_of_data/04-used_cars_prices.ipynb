{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Data Exercise #4 | Data Collection on Used Cars</h1>\n",
    "<p><b>4.</b> Collect data on used cars of a specific make and model (e.g., Ford Focus) in a city, from the Web,\n",
    "using a classified ads website or a used cars website as a source. Use web scraping to collect all\n",
    "available data on these cars. (Alternatively, collect the most important variables by hand from the\n",
    "100 most recently advertised cars.) Write a short report on what you did, how many cars you\n",
    "ended up with in the data, and what difficulties you encountered, if any. </p>\n",
    "<h3>Answer:</h3>\n",
    "<p>This was a very challenging exercise, technically speaking. I went though several tutorials and documentations to get the desired data. I'll describe part of the process together with a brief analytical content of it.</p>\n",
    "<p>My goal was to get all ads for <b>Hyundai</b> <b>HB20</b> models in the state of <b>Rio Grande do Sul</b>, <b>Brazil</b>. The extraction used in this notebook was performed in <b>March 7h, 2023</b>. I consulted the <a href='https://www.olx.com.br/'>OLX</a> online ad website to get the data (thank you OLX!).</p>\n",
    "<p>If you'd like to know how I scraped the data, please refer to the <code>.py</code> app stored in this same folder. I'll be commenting some of the difficulties that I'd faced when trying to develop this program.</p>\n",
    "<p>First, I'll import the required libraries and load the dataset.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_path = os.getcwd()\n",
    "data_in = f\"{current_path}/data/output/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cars = pd.read_csv(f\"{data_in}WS_car_prices.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(704, 11)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cars.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>As we can see, I managed to scrape 704 ads, all of them containing 11 variables that qualify each observed data. Let's take a look at each variable.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 704 entries, 0 to 703\n",
      "Data columns (total 11 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   car_model      704 non-null    object \n",
      " 1   car_price      704 non-null    float64\n",
      " 2   date           704 non-null    object \n",
      " 3   time           704 non-null    object \n",
      " 4   total_mileage  704 non-null    float64\n",
      " 5   year           704 non-null    int64  \n",
      " 6   gear_type      704 non-null    object \n",
      " 7   fuel_type      704 non-null    object \n",
      " 8   url            704 non-null    object \n",
      " 9   city           704 non-null    object \n",
      " 10  area           704 non-null    object \n",
      "dtypes: float64(2), int64(1), object(8)\n",
      "memory usage: 60.6+ KB\n"
     ]
    }
   ],
   "source": [
    "df_cars.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>As we can see, I scraped a good deal of information on each ad. Although the data must still go through an exploratory and a cleaning step, we can already observe that, besides a title for each ad, one can get the following information:\n",
    "<ul>\n",
    "    <li>price</li>\n",
    "    <li>date of the ad</li>\n",
    "    <li>time when the ad was updated</li>\n",
    "    <li>mileage</li>\n",
    "    <li>url for each ad</li>\n",
    "    <li>gear and fuel type</li>\n",
    "    <li>city and city area</li>\n",
    "</ul>\n",
    "\n",
    "<h4>Some of the difficulties that I'd faced during the web scraping process:</h4>\n",
    "The first problem was that, today, most ad websites work on dynamic content. That is, the content for each ad is dynamically updated via a backend js application, which updates the date after the webpage is accessed. Hence, traditionally-employed libraries such as <code>requests</code> only return empty values when accessed via a <code>BeautifulSoup</code> text parser. \n",
    "<b>The solution</b> came with a different library: </code>Selenium</code>, which is primarily used to run tests on a web browser. In my case, I used it to simulate a series of visits through each page of the website. For every page that the driver would open, the relevant data would be accessed, parsed, and stored in a <code>json</code>, which would be later converted to a <code>pandas DataFrame</code> so that I could adjust some issues and then export the extracted data to a <code>csv</code> file. Therefore, although I had to solve other problems (which are all documented in the app), the most difficult part was to actually get hold of the data; the rest was a series of smaller issues.</p>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "13bbb7fd5916020362212becdbb8a50aec9d9eb5bf2a3de6ebff663c4b0e188c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
